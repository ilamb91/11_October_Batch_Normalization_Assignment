{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6d50efd-476c-4e2f-afe1-145488371bb5",
   "metadata": {},
   "source": [
    "# Objective: The objective of this assignment is to assess students' understanding of batch normalization in artificial neural networks (ANN) and its impact on training performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91133b4-2bf8-4f09-ba94-e3ac2e6f5554",
   "metadata": {},
   "source": [
    "# Q1. Theory and Concepts:\n",
    "1. Explain the concept of batch normalization in the context of Artificial Neural Networks.\n",
    "2. Describe the benefits of using batch normalization during training.\n",
    "3. Discuss the working principle of batch normalization, including the normalization step and the learnable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6200fff2-78bd-46e8-87bc-8461251c84b5",
   "metadata": {},
   "source": [
    "**Batch Normalization (BatchNorm)** is a technique used in artificial neural networks to improve the training stability and speed by normalizing the input to each layer of a neural network. It was introduced by Sergey Ioffe and Christian Szegedy in their 2015 paper titled \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.\" BatchNorm is commonly applied to deep feedforward networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs).\n",
    "\n",
    "Here's an explanation of the concept, benefits, and working principles of Batch Normalization:\n",
    "\n",
    "**Concept of Batch Normalization**:\n",
    "In neural networks, the distribution of the input data to each layer can change as the network trains, a phenomenon known as \"internal covariate shift.\" This change in distribution can lead to slower training and the vanishing/exploding gradient problems. Batch Normalization addresses this issue by normalizing the input of each layer to have a mean of zero and a standard deviation of one.\n",
    "\n",
    "**Benefits of Using Batch Normalization**:\n",
    "\n",
    "1. **Accelerated Training**: BatchNorm significantly speeds up the training of neural networks. By maintaining more stable distributions throughout the training process, networks converge faster, which means they require fewer iterations to reach a desired level of performance.\n",
    "\n",
    "2. **Improved Gradient Flow**: BatchNorm mitigates the vanishing gradient problem, as normalizing the inputs ensures that the gradients don't become too small during backpropagation. This results in more stable and efficient training.\n",
    "\n",
    "3. **Regularization**: BatchNorm acts as a form of regularization. It introduces noise during training because the mean and standard deviation estimates are computed on mini-batches, which can help prevent overfitting.\n",
    "\n",
    "4. **Independence from Initialization**: Neural networks with BatchNorm are less sensitive to weight initialization. This makes it easier to train deep networks without requiring careful initialization strategies.\n",
    "\n",
    "**Working Principle of Batch Normalization**:\n",
    "\n",
    "BatchNorm operates on a mini-batch of data in the following steps:\n",
    "\n",
    "1. **Normalization**: For each feature (input dimension), BatchNorm computes the mean and standard deviation of that feature within the mini-batch. It then normalizes each feature by subtracting the mean and dividing by the standard deviation. This standardizes the distribution of each feature within the mini-batch.\n",
    "\n",
    "2. **Scaling and Shifting**: After normalization, BatchNorm introduces two learnable parameters, typically referred to as gamma (γ) and beta (β). These parameters allow the network to adapt the normalized values. They are applied to each feature, effectively scaling and shifting the values, allowing the network to recover the original representation if necessary.\n",
    "\n",
    "The equations for BatchNorm can be summarized as follows:\n",
    "\n",
    "Normalization step: \n",
    "\n",
    "\\[ \\hat{x}^{(k)} = \\frac{x^{(k)} - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\]\n",
    "\n",
    "Scaling and shifting step:\n",
    "\n",
    "\\[ y^{(k)} = \\gamma \\hat{x}^{(k)} + \\beta \\]\n",
    "\n",
    "Where:\n",
    "- \\(x^{(k)}\\) is the input feature.\n",
    "- \\(\\mu_B\\) is the mini-batch mean.\n",
    "- \\(\\sigma_B^2\\) is the mini-batch variance.\n",
    "- \\(\\epsilon\\) is a small constant to avoid division by zero.\n",
    "- \\(\\hat{x}^{(k)}\\) is the normalized input.\n",
    "- \\(\\gamma\\) is the scaling parameter (learnable).\n",
    "- \\(\\beta\\) is the shifting parameter (learnable).\n",
    "- \\(y^{(k)}\\) is the final output.\n",
    "\n",
    "In practice, BatchNorm layers are inserted before or after the activation functions in a neural network, and the gamma and beta parameters are learned during training via backpropagation.\n",
    "\n",
    "In summary, Batch Normalization is a crucial technique in deep learning that normalizes the inputs to neural network layers to improve training stability, speed, and regularization. It addresses issues related to internal covariate shift and helps in achieving faster convergence and better generalization in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ef6a03-f2dc-4350-9357-848b1fc70fdb",
   "metadata": {},
   "source": [
    "# Q2. Implementation:\n",
    "1. Choose a dataset of your choice (e.g., MNIST, CIFAR-10) and preprocess it.\n",
    "2. Implement a simple feedforward neural network using any deep learning framework/library (e.g., TensorFlow, PyTorch).\n",
    "3. Train the neural network on the chosen dataset without using batch normalization.\n",
    "4. Implement batch normalization layers in the neural network and train the model again.\n",
    "5. Compare the training and validation performance (e.g, accuracy, loss) between the models with and without batch normalization.\n",
    "6. Discuss the impact of batch normalization on the training process and the performance of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaa3c93-d463-4dfe-9d6c-8d1b475e462b",
   "metadata": {},
   "source": [
    "The process of comparing the training performance of a feedforward neural network with and without batch normalization using the popular MNIST dataset. In this example, we'll use PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90ed0131-ea96-4a50-856e-8eb80a52d619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.10/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (2.8.8)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2022.11.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.2.140)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4025e101-a3cd-4aad-8a6a-a9d83cdece1b",
   "metadata": {},
   "source": [
    "Here are the steps to create and train two models, one without batch normalization and one with batch normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8712d966-dcbe-48fa-b6aa-e0243f90eeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (2.8.8)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.10/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2022.11.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.2.140)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba3f5ff6-9b79-4629-a531-7c903548d101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model without batch normalization:\n",
      "Epoch 1, Loss: 0.36867520165468837\n",
      "Epoch 2, Loss: 0.1822503679739768\n",
      "Epoch 3, Loss: 0.13570476690136485\n",
      "Epoch 4, Loss: 0.11102820791677435\n",
      "Epoch 5, Loss: 0.09394661429475175\n",
      "Epoch 6, Loss: 0.08266445356599891\n",
      "Epoch 7, Loss: 0.07139173642283421\n",
      "Epoch 8, Loss: 0.06404634101414627\n",
      "Epoch 9, Loss: 0.059319895748763896\n",
      "Epoch 10, Loss: 0.051545023099521296\n",
      "Accuracy without batch normalization: 97.27%\n",
      "Training the model without batch normalization:\n",
      "Epoch 1, Loss: 0.2768898174119021\n",
      "Epoch 2, Loss: 0.13548415673098393\n",
      "Epoch 3, Loss: 0.10077079324357545\n",
      "Epoch 4, Loss: 0.08079860943145176\n",
      "Epoch 5, Loss: 0.06692490836323452\n",
      "Epoch 6, Loss: 0.05706651926612549\n",
      "Epoch 7, Loss: 0.04970864901867217\n",
      "Epoch 8, Loss: 0.044404099378655394\n",
      "Epoch 9, Loss: 0.03742729922779389\n",
      "Epoch 10, Loss: 0.03472570752127291\n",
      "Training the model with batch normalization:\n",
      "Epoch 1, Loss: 0.2776558626490806\n",
      "Epoch 2, Loss: 0.13658219587796533\n",
      "Epoch 3, Loss: 0.09931230906353418\n",
      "Epoch 4, Loss: 0.08102635582233829\n",
      "Epoch 5, Loss: 0.06704975230726543\n",
      "Epoch 6, Loss: 0.058074414628798136\n",
      "Epoch 7, Loss: 0.049000901363686775\n",
      "Epoch 8, Loss: 0.04332847005602266\n",
      "Epoch 9, Loss: 0.03812856089744741\n",
      "Epoch 10, Loss: 0.03342624892890731\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 96\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model with batch normalization\u001b[39;00m\n\u001b[1;32m     95\u001b[0m model_with_batchnorm \u001b[38;5;241m=\u001b[39m train_network(use_batchnorm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 96\u001b[0m accuracy_with_batchnorm \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_with_batchnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy with batch normalization: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_with_batchnorm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Step 6: Discuss the impact of batch normalization\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 79\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     78\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m---> 79\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     81\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Preprocess the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define a function to create and train a simple feedforward neural network\n",
    "def train_network(use_batchnorm):\n",
    "    # Step 2: Define the neural network\n",
    "    class SimpleNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(SimpleNN, self).__init__()\n",
    "            self.fc1 = nn.Linear(28*28, 128)\n",
    "            if use_batchnorm:\n",
    "                self.bn1 = nn.BatchNorm1d(128)  # Batch normalization layer\n",
    "            self.relu = nn.ReLU()\n",
    "            self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.view(-1, 28*28)\n",
    "            x = self.fc1(x)\n",
    "            if use_batchnorm:\n",
    "                x = self.bn1(x)  # Apply batch normalization\n",
    "            x = self.relu(x)\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "    # Step 3: Train the neural network without batch normalization\n",
    "    net = SimpleNN()\n",
    "    if use_batchnorm:\n",
    "        net_with_batchnorm = SimpleNN()  # Create a network with batch normalization\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    if use_batchnorm:\n",
    "        optimizer_with_batchnorm = optim.SGD(net_with_batchnorm.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    def train_model(net, train_loader, optimizer, use_batchnorm):\n",
    "        for epoch in range(10):\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(train_loader, 0):\n",
    "                inputs, labels = data\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {running_loss / (i + 1)}\")\n",
    "\n",
    "    print(\"Training the model without batch normalization:\")\n",
    "    train_model(net, train_loader, optimizer, use_batchnorm=False)\n",
    "\n",
    "    # Step 4: Train the neural network with batch normalization\n",
    "    if use_batchnorm:\n",
    "        print(\"Training the model with batch normalization:\")\n",
    "        train_model(net_with_batchnorm, train_loader, optimizer_with_batchnorm, use_batchnorm=True)\n",
    "\n",
    "    # Step 5: Compare training and validation performance\n",
    "    if use_batchnorm:\n",
    "        return net, net_with_batchnorm\n",
    "    else:\n",
    "        return net\n",
    "\n",
    "# Step 6: Discuss the impact of batch normalization\n",
    "def evaluate_model(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Train and evaluate the model without batch normalization\n",
    "model_no_batchnorm = train_network(use_batchnorm=False)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "accuracy_no_batchnorm = evaluate_model(model_no_batchnorm, test_loader)\n",
    "print(f\"Accuracy without batch normalization: {accuracy_no_batchnorm}%\")\n",
    "\n",
    "# Train and evaluate the model with batch normalization\n",
    "model_with_batchnorm = train_network(use_batchnorm=True)\n",
    "accuracy_with_batchnorm = evaluate_model(model_with_batchnorm, test_loader)\n",
    "print(f\"Accuracy with batch normalization: {accuracy_with_batchnorm}%\")\n",
    "\n",
    "# Step 6: Discuss the impact of batch normalization\n",
    "print(\"Impact of Batch Normalization:\")\n",
    "print(\"Accuracy without batch normalization:\", accuracy_no_batchnorm)\n",
    "print(\"Accuracy with batch normalization:\", accuracy_with_batchnorm)\n",
    "\n",
    "# Visualize the weights of the first layer\n",
    "def visualize_weights(model):\n",
    "    layer = model.fc1\n",
    "    weights = layer.weight.data\n",
    "    for i in range(10):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        weight = weights[i].view(28, 28)\n",
    "        plt.imshow(weight, cmap='viridis')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the weights of the first layer for the model without batch normalization\n",
    "visualize_weights(model_no_batchnorm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeb60eb-36da-4326-b50c-9dfc68969ee3",
   "metadata": {},
   "source": [
    "# Q3. Experimentation and Analysis:\n",
    "1. Experiment with different batch sizes and observe the effect on the training dynamics and model performance.\n",
    "2. Discuss the advantages and potential limitations of batch normalization in improving the training of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d2841-70d2-47ae-acba-b838f90f99c1",
   "metadata": {},
   "source": [
    "A3.\n",
    "\n",
    "**Experimenting with Batch Sizes:**\n",
    "\n",
    "1. **Effect of Batch Size on Training Dynamics:**\n",
    "   - Smaller Batch Sizes: Training with smaller batch sizes can result in more noisy gradients and slower convergence. This is because the updates are based on fewer examples, and the direction of the gradient may be less accurate.\n",
    "   - Larger Batch Sizes: Training with larger batch sizes can provide smoother gradients, potentially leading to faster convergence. However, this comes at the cost of increased memory and computation requirements.\n",
    "\n",
    "2. **Effect on Model Performance:**\n",
    "   - Smaller Batch Sizes: Smaller batch sizes might lead to more significant fluctuations in training loss and can make the model generalize better, particularly in cases where larger batch sizes tend to converge to suboptimal solutions.\n",
    "   - Larger Batch Sizes: Larger batch sizes may result in more stable training dynamics and better generalization, especially when the data is noisy.\n",
    "\n",
    "3. **Computational Efficiency:**\n",
    "   - Smaller Batch Sizes: Smaller batch sizes consume less memory but can be computationally less efficient because of the overhead associated with frequent weight updates.\n",
    "   - Larger Batch Sizes: Larger batch sizes can be more computationally efficient as they make better use of parallelism in hardware, but they require more memory.\n",
    "\n",
    "**Advantages and Potential Limitations of Batch Normalization:**\n",
    "\n",
    "Advantages:\n",
    "\n",
    "1. **Improved Convergence and Training Speed**: Batch normalization can accelerate training by mitigating issues like vanishing gradients and allowing for faster convergence. This is especially beneficial for deep networks.\n",
    "\n",
    "2. **Improved Generalization**: It can act as a form of regularization, reducing the risk of overfitting. The noise introduced during mini-batch statistics estimation can improve generalization.\n",
    "\n",
    "3. **Reduced Sensitivity to Initialization**: Batch normalization reduces the sensitivity of neural networks to weight initialization. This makes it easier to train deep networks and explore different architectures.\n",
    "\n",
    "4. **Stabilizes Activation Distributions**: Batch normalization normalizes the activations of each layer, which helps to maintain consistent and stable distributions throughout the network. This can be particularly useful in very deep networks.\n",
    "\n",
    "Potential Limitations:\n",
    "\n",
    "1. **Increased Memory and Computational Requirements**: Batch normalization introduces additional parameters (scale and shift) and requires storing and updating statistics for each batch. This can increase memory and computation demands.\n",
    "\n",
    "2. **Not Always Beneficial**: In some cases, batch normalization may not provide significant advantages or could even lead to degradation in performance. It's not a one-size-fits-all solution.\n",
    "\n",
    "3. **Dependence on Batch Size**: The effectiveness of batch normalization can depend on the choice of batch size. Very small batch sizes may result in less accurate statistics estimates, while very large batch sizes may lose some of the regularization benefits.\n",
    "\n",
    "4. **Not Suitable for Recurrent Networks**: Batch normalization's original formulation is not well-suited for recurrent neural networks (RNNs) due to the temporal dependencies in sequences. Variants like Layer Normalization and Group Normalization are used in such cases.\n",
    "\n",
    "In summary, batch normalization is a valuable technique for training deep neural networks, offering faster convergence and better generalization. However, its effectiveness can vary with batch size, and it introduces additional computational costs and model complexity. The choice of using batch normalization should depend on the specific problem, architecture, and hardware constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dfb14d-f7bd-41fb-991d-b867c3bf9395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
